# ndis_agent
To keep things simple, I have built this agent based on information on one webpage only.

<a target="_blank" href="https://cookiecutter-data-science.drivendata.org/">
    <img src="https://img.shields.io/badge/CCDS-Project%20template-328F97?logo=cookiecutter" />
</a>

This Agent will aim to answer questions on NDIS participants

## Project Organization

```
├── LICENSE            <- Open-source license if one is chosen
├── Makefile           <- Makefile with convenience commands like `make data` or `make train`
├── README.md          <- The top-level README for developers using this project.
├── data
│   ├── external       <- Data from third party sources.
│   ├── interim        <- Intermediate data that has been transformed.
│   ├── processed      <- The final, canonical data sets for modeling.
│   └── raw            <- The original, immutable data dump.
│
├── docs               <- A default mkdocs project; see www.mkdocs.org for details
│
├── models             <- Trained and serialized models, model predictions, or model summaries
│
├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
│                         the creator's initials, and a short `-` delimited description, e.g.
│                         `1.0-jqp-initial-data-exploration`.
│
├── pyproject.toml     <- Project configuration file with package metadata for 
│                         ndis_agent and configuration for tools like black
│
├── references         <- Data dictionaries, manuals, and all other explanatory materials.
│
├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
│   └── figures        <- Generated graphics and figures to be used in reporting
│
├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
│                         generated with `pip freeze > requirements.txt`
│
├── setup.cfg          <- Configuration file for flake8
│
└── ndis_agent   <- Source code for use in this project.
    │
    ├── __init__.py             <- Makes ndis_agent a Python module
    │
    ├── config.py               <- Store useful variables and configuration
    │
    ├── dataset.py              <- Scripts to download or generate data
    │
    ├── features.py             <- Code to create features for modeling
    │
    ├── modeling                
    │   ├── __init__.py 
    │   ├── predict.py          <- Code to run model inference with trained models          
    │   └── train.py            <- Code to train models
    │
    └── plots.py                <- Code to create visualizations
```

# Setting Up Your Llama 3 Q&A System

This guide will walk you through setting up a complete Q&A system that can answer questions from your large text files using Meta's powerful Llama 3 model - all locally on your machine with no API costs.

## Step 1: Install Required Python Libraries

First, create a new Python environment and install the necessary libraries:

```bash
# Create and activate a virtual environment (recommended)
python -m venv ndis_qa_env
source ndis_qa_env/bin/activate  # On Windows: ndis_qa_env\Scripts\activate

# Install required packages
pip install langchain langchain-community faiss-cpu sentence-transformers streamlit
```

## Step 2: Install Ollama

Ollama is a tool that makes it easy to run large language models like Llama 3 locally.

### On macOS or Linux:
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### On Windows:
1. Download the installer from [https://ollama.com/download/windows](https://ollama.com/download/windows)
2. Run the installer and follow the prompts

## Step 3: Download and Install Llama 3

After installing Ollama, download the Llama 3 model:

```bash
# Open a terminal/command prompt and run:
ollama pull llama3
```

This will download the model (about 4GB). Wait for it to complete.

## Step 5: Run the Application

Make sure Ollama is running in the background, then launch the Streamlit interface:

```bash
streamlit run 04.rag_app.py
```

The web interface will open in your browser.

## Step 6: Using the Q&A System

2. The system will process all the information from the ndis webpage, creating chunks and building a vector index using FAISS
3. Once processing is complete, you can start asking questions
4. Type your questions in the chat input and get answers generated by Llama 3

## Advanced Options

### Command Line Usage

You can also use the system directly from the command line:

```bash
python 03.qanda_rag.py --interactive
```

### Saving and Loading Indexes

To avoid reprocessing large files, you can save the vector index:
1. In the web interface, use the "Save Index" button in the sidebar
2. Give your index a name
3. Later, use "Load Index" to reload it without reprocessing the file

### Performance Optimization

- For better performance, consider running on a machine with a GPU
- If you have sufficient GPU memory (16GB+), you can try the larger 70B parameter model:
  ```bash
  ollama pull llama3:70b
  ```
  Then modify the code to use `llama3:70b` instead of just `llama3`

## Troubleshooting

- If you encounter CUDA/GPU errors, try forcing CPU usage
- If the model is too slow, try reducing chunk sizes or using a smaller embedding model
- If answers lack detail, try increasing the number of chunks retrieved (k_retrieval parameter)

--------

